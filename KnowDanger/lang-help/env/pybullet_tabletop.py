"""
Tabletop manipulation simulation in PyBullet.

Based on the SayCan colab from Andy.

"""
import argparse
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import imageio
import itertools
from omegaconf import OmegaConf

from env.pick_place_env import PickPlaceEnv
from env.detection.vild import ViLD


home_path = str(Path.home())

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # add a flag for using gui or not
    parser.add_argument(
        "--gui",
        action="store_true",
        help="Whether to use PyBullet GUI or not",
    )
    args = parser.parse_args()

    # if 'env' in locals():
    #   # Safely exit gripper threading before re-initializing environment.
    #   env.gripper.running = False
    #   while env.gripper.constraints_thread.isAlive():
    #     time.sleep(0.01)
    camera_param = OmegaConf.create()
    camera_param.noise = False
    env = PickPlaceEnv(render=args.gui, camera_param=camera_param)

    # Define and reset environment.
    config = {
        # 'pick': ['yellow circle', 'green circle', 'blue circle'],
        # 'place': ['yellow block', 'green block', 'blue block']
        'pick': ['yellow circle'],
        'place': []
    }

    np.random.seed(42)
    obs = env.reset(config)
    before = env.get_camera_image()
    prev_obs = obs['image'].copy()

    # pick_xyz = [0, -0.2, 0.1]
    # place_xyz = [-0.2, 0.2, 0.1]

    # # Step environment.
    # act = {'pick': pick_xyz, 'place': place_xyz}
    # print('act', act)
    # obs, _, _, _ = env.step(act)

    fig = plt.figure(figsize=(20, 10))
    plt.subplot(1, 2, 1)
    img = env.get_camera_image()
    plt.title('Perspective side-view')
    plt.imshow(img)
    plt.subplot(1, 2, 2)
    img = env.get_camera_image_top()
    img = np.flipud(img.transpose(1, 0, 2))
    plt.title('Orthographic top-view')
    plt.imshow(img)
    plt.show()
    # image_path = 'env/test/tmp.jpg'
    # imageio.imwrite(image_path, img)

    # Note: orthographic cameras do not exist. But we can approximate them by projecting a 3D point cloud from an RGB-D camera, then unprojecting that onto an orthographic plane. Orthographic views are useful for spatial action maps.
    plt.title('Unprojected orthographic top-view')
    plt.imshow(obs['image'])
    plt.show()
    image_path = 'env/test/tmp.jpg'
    imageio.imwrite(image_path, img)

    # Get ViLD detections.
    detector = ViLD()
    # 'purple', 'pink', 'cyan', 'orange', 'pentagon'
    object_names = [
        'block',
        'circle',
        'triangle',
        'star',
    ]  # 'bowl'
    color_names = [
        'blue',
        'red',
        'green',
        'yellow',
    ]
    category_names = itertools.product(color_names, object_names)
    category_names = [f'{c} {o}' for c, o in category_names]

    # Extra prompt engineering: swap A with B for every (A, B) in list.
    prompt_swaps = [
        # ('block', 'cube'),
        # ('bowl', 'dish'),
    ]

    # markdown ViLD settings.
    category_name_string = ";".join(category_names)
    max_boxes_to_draw = 6  #@param {type:"integer"}
    nms_threshold = 0.4  #@param {type:"slider", min:0, max:0.9, step:0.05}
    min_rpn_score_thresh = 0.4  #@param {type:"slider", min:0, max:1, step:0.01}
    min_box_area = 100
    max_box_area = 1000
    vild_params = max_boxes_to_draw, nms_threshold, min_rpn_score_thresh, min_box_area, max_box_area
    found_objects, boxes = detector.infer(
        image_path,
        category_name_string,
        vild_params,
        prompt_swaps=prompt_swaps,
        display_img=True,
    )
    # found_objects = [
    #     'yellow bowl', 'green bowl', 'cyan bowl', 'yellow block', 'blue block',
    #     'green block', 'blue block'
    # ]
    # boxes = [[140.1936, 141.36359, 186.65874, 185.91174],
    #          [89.856316, 56.025673, 136.40218, 101.91679],
    #          [38.267918, 91.71067, 55.895496, 108.34257],
    #          [95.56264, 148.55998, 111.93001, 165.57666],
    #          [164.63315, 55.693043, 184.09561, 73.32377],
    #          [159.80367, 47.63284, 194.22485, 82.22621],
    #          [160.07808, 71.356384, 167.52107, 79.80932]]
    print(found_objects, boxes)
    while 1:
        pass

    # Assume we already get the pick-place object and location from the action generated by LM
    pick_obj = 'blue block'
    place_pos = 'yellow bowl'
    pick_box = boxes[found_objects.index(pick_obj)]
    place_box = boxes[found_objects.index(place_pos)]

    # box and xyzmap are aligned
    # box follows image convention: the first and third coordinates are the y coordinates (down), and the second and fourth coordinates are the x coordinates (right)
    # xyzmap has first axis with down as positive y, and second axis as right as positive x. The robot is at origin.
    # xyzmap boundaries: top left - [-0.3 -0.2  0. ]; top right - [0.3 -0.2  0. ]; bottom left - [-0.3 -0.8 0.00621685]; bottom right - [0.3 -0.8 0.00547853]

    # Get pick position.
    pick_yx = [
        int((pick_box[0] + pick_box[2]) / 2),
        int((pick_box[1] + pick_box[3]) / 2)
    ]
    pick_yx[
        0
    ] -= 10  # TODO: had to move the gripper a bit in negative y (closer to robot)
    pick_yx[1] -= 5
    pick_xyz = obs['xyzmap'][pick_yx[0], pick_yx[1]]

    # Get place position.
    place_yx = [
        int((place_box[0] + place_box[2]) / 2),
        int((place_box[1] + place_box[3]) / 2)
    ]
    place_yx[0] -= 10
    place_yx[1] -= 5
    place_xyz = obs['xyzmap'][place_yx[0], place_yx[1]]

    # Step environment.
    act = {'pick': pick_xyz, 'place': place_xyz}
    print('act', act)
    obs, _, _, _ = env.step(act)

    # Show pick and place action.
    plt.imshow(prev_obs)
    plt.arrow(
        pick_yx[1],
        pick_yx[0],
        place_yx[1] - pick_yx[1],
        place_yx[0] - pick_yx[0],
        color='w',
        head_starts_at_zero=False,
        head_width=7,
        length_includes_head=True,
    )
    plt.show()

    # # Show video of environment rollout.
    # if display_video:
    #     debug_clip = ImageSequenceClip(env.cache_video, fps=25)
    #     display(debug_clip.ipython_display(autoplay=1, loop=1, center=False))
    # env.cache_video = []

    # Show camera image after pick and place.
    plt.subplot(1, 2, 1)
    plt.title('Before')
    plt.imshow(before)
    plt.subplot(1, 2, 2)
    plt.title('After')
    after = env.get_camera_image()
    plt.imshow(after)
    plt.show()
