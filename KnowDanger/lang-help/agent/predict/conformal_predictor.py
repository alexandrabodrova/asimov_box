""" Run conformal prediction for single-step template data (i.e., multiple choices generated with templates, instead of by prompting LM).

This loads the data of LM answering the multiple choice questions generated by collect_data.py.

The results include (1) average prediction set size, (2) empirical coverage, (3) help frequency, and (4) success ratio.

Multiple methods are implemented:
    - naive:
        - include choices from higher scores to lower ones until the coverage is reached
        - no calibration procedure
    - conformal:
        - use the score function (1-softmax(true label)) and the quantile in the calibration dataset to determine the prediction set at test time. Also known as LABEL.
    - conformal_top_k:
        - same as conformal, but use the rank of the true label, with scores ranked from higher to lower, as the score function
        - this results the same prediction set size for all test data.
    - adaptive_conformal:
        - same as conformal, but the score is defined by summing the ranked scores of each label, from the higher to the lower until reaching the true label of the observation
    - regularized_adaptive_conformal: regularized adaptive conformal prediction (RAPS)
        - same as adaptive_conformal, but the rank scores of the labels are regularized

Reference: https://mapie.readthedocs.io/en/latest/theoretical_description_classification.html

"""
import os
import argparse
import logging
import numpy as np
from omegaconf import OmegaConf
from agent.predict.base_predictor import BasePredictor
from agent.predict.util import get_score, get_prediction_set, temperature_scaling


class ConformalPredictor(BasePredictor):

    def __init__(self, cfg):
        super().__init__(cfg)

        # Empirical quantile level
        self.q_level = np.ceil((self.n + 1) * (1 - cfg.alpha)) / self.n

    def calibrate(self, plot_fig=False, log=True):
        cfg = self.cfg
        bad_score_true_label = []
        cal_scores = []
        for data_ind, data in enumerate(self.calibration_data):

            # Get information
            true_label = data['true_label']
            top_tokens = data['top_tokens']
            top_logprobs = data['top_logprobs']

            # temperature scaling and get softmax
            if 'top_smx' in data:  # from ensemble
                top_smx = np.array(data['top_smx'])
            else:
                top_smx = temperature_scaling(
                    top_logprobs, cfg.temperature_scaling
                )

            # nucleus sampling
            print(top_smx)
            if cfg.nucleus_threshold > 0:

                # sort top_smx and top_tokens
                order = np.argsort(top_smx)[::-1]
                top_tokens = [top_tokens[i] for i in order]
                top_smx = np.array([top_smx[i] for i in order])

                # find the cumulative probability that exceeds the threshold
                cum_smx = np.cumsum(top_smx)
                print(cum_smx)
                # find the first index that exceeds the threshold
                ind = np.where(cum_smx > cfg.nucleus_threshold)[0][0]
                print(ind)
                # truncate the distribution
                top_smx[(ind + 1):] = 0
                print(top_smx)
                top_smx = temperature_scaling(top_smx, 1)
            print(top_smx)
            print()

            # For multi-label setting: choose the true label one with the highest score
            if cfg.multi_label:
                assert len(true_label) > 0  # should be a list
                try:
                    true_label_smx = [
                        top_smx[sig_ind]
                        for sig_ind, sig in enumerate(top_tokens)
                        if sig in true_label
                    ]
                except:
                    breakpoint()
                true_label = true_label[np.argmax(true_label_smx)]
            else:
                # human provides a list - remove list
                if isinstance(true_label, list):
                    assert len(true_label) == 1
                    true_label = true_label[0]

            # Get calibration score
            cal_score = get_score(
                top_tokens,
                top_smx,
                true_label,
                cfg.score_method,
                cfg,
            )
            cal_scores.append(cal_score)

            # Debug: print examples with bad (high) score
            # if cal_score > 0.8:
            #     logging.info(data['request'])
            #     logging.info(data['mc_prompt'])
            #     logging.info(data['true_action'])
            #     logging.info(
            #         f'True label: {true_label}; Softmax: {top_smx}; Top tokens: {top_tokens}'
            #     )
            #     bad_score_true_label.append(true_label)

        # Empirical quantile
        self.qhat = np.quantile(cal_scores, self.q_level, method='higher')

        # Plot score histogram
        if plot_fig:
            self.plot_score_histogram(cal_scores, cfg)

        # Report
        if log:
            logging.info('============== Summary ==============')
            logging.info('Number of calibration data: %d', self.n)
            logging.info('Quantile level: %.3f', self.q_level)
            logging.info('Quantile value: %.5f', self.qhat)
            # logging.info(
            #     'Bad score true label split (A,B,C,D,E): %s', [
            #         len([i
            #              for i in bad_score_true_label
            #              if i == j])
            #         for j in cfg.mc_sigs
            #     ]
            # )

    def test(self, plot_fig=False, log=False):
        cfg = self.cfg
        prediction_set_size = []
        # prediction_set_size_per_type = {'eq': [], 'amb': [], 'sem': []}
        num_correct_prediction_set = 0
        num_help = 0
        num_success = 0
        num_weird = 0
        smx_residual_all = []

        # Loop over test data
        for data in self.test_data:

            # Get information
            true_label = data['true_label']
            top_tokens = data['top_tokens']
            top_logprobs = data['top_logprobs']
            if 'add_mc_prefix' in data:
                none_option_token = data['add_mc_prefix']
            else:
                none_option_token = cfg.e_sig  # default to 'e/E'
            print(top_tokens)
            # Make sure true label is a list
            if not isinstance(true_label, list):
                true_label = [true_label]

            # temperature scaling and get softmax
            if 'top_smx' in data:  # from ensemble
                top_smx = np.array(data['top_smx'])
            else:
                top_smx = temperature_scaling(
                    top_logprobs, cfg.temperature_scaling
                )

            # nucleus sampling
            print(top_smx)
            if cfg.nucleus_threshold > 0:

                # sort top_smx and top_tokens
                order = np.argsort(top_smx)[::-1]
                top_tokens = [top_tokens[i] for i in order]
                top_smx = np.array([top_smx[i] for i in order])

                # find the cumulative probability that exceeds the threshold
                cum_smx = np.cumsum(top_smx)
                print(cum_smx)
                # find the first index that exceeds the threshold
                ind = np.where(cum_smx > cfg.nucleus_threshold)[0][0]
                print(ind)
                # truncate the distribution
                top_smx[(ind + 1):] = 0
                print(top_smx)
                top_smx = temperature_scaling(top_smx, 1)
            print(top_smx)
            print(top_tokens)
            print(true_label)

            # Get prediction set
            prediction_set = get_prediction_set(
                top_tokens, top_smx, self.qhat, cfg.score_method, cfg
            )
            print(prediction_set)
            print()

            # ask for help if prediction set is (1) not a singleton or (2) equal to ['E']
            flag_help = False
            if cfg.count_e_as_help:
                cond = len(
                    prediction_set
                ) != 1 or none_option_token in prediction_set
            else:
                cond = len(prediction_set) != 1
            if cond:
                num_help += 1
                flag_help = True

            # check success
            true_label_in_prediction_set = not set(true_label
                                                  ).isdisjoint(prediction_set)
            if flag_help:
                # success if (1) true label is in prediction set (true label can also be E, then human provides aciton) or
                # (2) prediction set is empty (human provides aciton)
                if cfg.help_mode == 'from_prediction_set':
                    num_success += true_label_in_prediction_set or len(
                        prediction_set
                    ) == 0
                    # from copy import copy
                    # true_label_exclude_none = copy(true_label)
                    # # remove none option token
                    # if none_option_token in true_label_exclude_none:
                    #     true_label_exclude_none.remove(none_option_token)
                    # if none_option_token in prediction_set and set(
                    #     true_label_exclude_none
                    # ).isdisjoint(
                    #     prediction_set
                    # ):  # the only true label in the prediction set is E
                    #     num_weird += 1
                elif cfg.help_mode == 'from_all_mc':  # always work
                    num_success += 1
                else:
                    raise 'Unknown help model!'
            else:
                assert len(prediction_set) == 1
                num_success += true_label_in_prediction_set

            # Check if true label is in prediction set
            num_correct_prediction_set += true_label_in_prediction_set
            prediction_set_size.append(len(prediction_set))
            # prediction_set_size_per_type[true_type].append(len(prediction_set))

            # Log if specified
            # if log:
            #     logging.info('----------------------------------------')
            #     logging.info(data['request'])
            #     logging.info(data['mc_prompt'])
            #     logging.info(
            #         f'True label: {true_label}; Prediction set: {prediction_set}'
            #     )
            #     logging.info(
            #         f'Smx: {[np.round(top_smx[top_tokens.index(sig)], 3) for sig in cfg.mc_sigs if sig in top_tokens]}'
            #     )
            #     logging.info('----------------------------------------\n')

        # Plot prediction set histogram - wider is better
        if plot_fig:
            self.plot_prediction_set_size(prediction_set_size, cfg)

        # counts of different set size from 0 to 5
        # print(prediction_set_size)
        prediction_set_size_cnt = {
            '0': 0,
            '1': 0,
            '2': 0,
            '3': 0,
            '4': 0,
            '5': 0
        }
        for i in prediction_set_size:
            if i > 5:
                prediction_set_size_cnt['5'] += 1
            else:
                prediction_set_size_cnt[str(i)] += 1
        print(prediction_set_size_cnt)

        # Summarize results
        avg_prediction_set_size = np.mean(prediction_set_size)
        empirical_coverage = num_correct_prediction_set / (
            self.num_data - self.n
        )
        help_freq = num_help / len(self.test_data)
        success_ratio = num_success / len(self.test_data)

        # Report
        if log:
            logging.info('============== Summary ==============')
            logging.info('Number of calibration data: %d', self.n)
            logging.info('Number of test data: %d', self.num_data - self.n)
            # logging.info('Quantile level: %.3f', q_level)
            # logging.info('Quantile value: %.5f', self.qhat)
            logging.info(
                'Average prediction set size: %.3f',
                np.mean(prediction_set_size)
            )
            # logging.info(
            #     'Average prediction set size per type (eq, amb, sem): %s', [
            #         np.mean(prediction_set_size_per_type[key])
            #         if len(prediction_set_size_per_type[key]) > 0 else 0
            #         for key in prediction_set_size_per_type.keys()
            #     ]
            # )
            logging.info('Marginal coverage: %.3f', 1 - cfg.alpha)
            logging.info('Empirical coverage: %.3f', empirical_coverage)
            logging.info('Number of help: %d', num_help)
            logging.info('Number of success: %d', num_success)
            logging.info('Help frequency: %.3f', help_freq)
            logging.info('Success ratio: %.3f', success_ratio)
            logging.info('Number of weird: %d', num_weird)

        return avg_prediction_set_size, empirical_coverage, help_freq, success_ratio


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-cf", "--cfg_file", help="cfg file path", default='', type=str
    )
    args = parser.parse_args()
    cfg = OmegaConf.load(args.cfg_file)
    cfg.save_dir = os.path.dirname(args.cfg_file)
    cfg.logging_path = os.path.join(cfg.save_dir, 'calibration.log')

    # logging
    if cfg.logging_path is not None:
        logging.basicConfig(
            level=logging.INFO, format='%(message)s', handlers=[
                logging.FileHandler(cfg.logging_path, mode='w'),
                logging.StreamHandler()
            ]
        )  # overwrite

    # Agent
    agent = ConformalPredictor(cfg)

    # # for naive method, first find the calibration level so that Naive achieves empirical level on calibration set
    # if cfg.score_method == 'naive':
    #     target_coverage = cfg.naive_cal_level
    #     naive_cal_level_all = np.arange(0.3, 0.99, 0.03)
    #     naive_cal_level_all = np.hstack(
    #         (naive_cal_level_all, np.arange(0.991, 0.999, 0.001))
    #     )
    #     prev_empirical_coverage = 0
    #     for naive_cal_level in naive_cal_level_all:
    #         cfg.score_method = 'naive'
    #         cfg.naive_cal_level = float(naive_cal_level)
    #         cfg.temperature_scaling = cfg.temperature_scaling
    #         agent = ConformalPredictor(cfg)
    #         agent.calibrate(log=False)
    #         prediction_set_size, empirical_coverage, _, _ = agent.test()
    #         if empirical_coverage > target_coverage:
    #             break
    #         prev_empirical_coverage = empirical_coverage
    #     logging.info(
    #         'Naive cal level: {} for alpha {}. Empirical coverage: {}'.format(
    #             naive_cal_level, cfg.alpha, empirical_coverage
    #         )
    #     )

    # Calibrate
    agent.calibrate(plot_fig=cfg.plot_fig)

    # Test - often we only calibrate
    if cfg.test:
        agent.test(plot_fig=cfg.plot_fig, log=(cfg.logging_path is not None))