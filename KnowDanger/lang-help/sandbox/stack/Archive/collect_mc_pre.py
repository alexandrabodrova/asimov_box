""" Multi-step, human clarification, partially observable, tabletop manipulation environment

Process multiple chocies generated by LM, and prepare prompt for LM to answer.

"""

import os
import argparse
import pickle
import logging
import random
from omegaconf import OmegaConf

from agent.multiple_choice import MultipleChoice


def main(cfg):

    # Load previous prompt data
    with open(cfg.prev_prompt_path, 'rb') as f:
        prev_prompt_all = pickle.load(f)

    # Load LM response
    with open(cfg.lm_response_path, 'r') as f:
        lm_response = f.read()
    lm_response_all = lm_response.split('--0000--')
    assert len(lm_response_all) == len(prev_prompt_all)

    # Multiple choice agent
    mc_agent = MultipleChoice()

    # Generate data
    num_data = len(prev_prompt_all)
    data_all = []
    for data_ind, (prev_prompt, lm_response) in enumerate(
        zip(prev_prompt_all, lm_response_all)
    ):
        print(prev_prompt['prompt'])
        print(lm_response)

        # Process LM response
        mc_action_all, success = mc_agent.process_multiple_choice(lm_response)

        # Prompt true label from human
        # while 1:
        #     try:
        #         # logging.warning(f'Again, the task is {request}')
        #         # logging.warning(
        #         #     f'And the ground truth is {info.request_unambiguous}'
        #         # )
        #         true_label = input(
        #             "Please provide label(s) in the format of 'label_1, label_2, ...'; E for none of the above: "
        #         ).split(',')
        #         if len(true_label) < 1:
        #             raise ValueError
        #     except:
        #         continue
        #     break
        r = input('Press a to continue...')
        if r == 'q':
            break
        elif r == 'a':
            continue

        # Get new prompt
        prompt = prev_prompt['prompt'] + mc_action_all + '\n\n' \
            + cfg.answer_for_action_prompt

        # Log
        logging.info(
            '============ Data {}, new prompt ============'.
            format(data_ind + 1)
        )
        logging.info(prompt)
        logging.info(
            f'================= END {data_ind+1}/{num_data} =================\n\n'
        )

        # Save data
        data_all.append({
            # 'true_label': true_label,
            'lm_mc_response': lm_response,
            'prompt': prompt,
        })

    # Save all data
    with open(cfg.data_save_path, 'wb') as f:
        pickle.dump(data_all, f)

    # Summary
    logging.info('\n============== Summary ==============')
    logging.info(f'Number of questions generated: {len(data_all)}')
    logging.info(f'Data saved to: {cfg.data_save_path}.')
    logging.info('=====================================')


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-cf", "--cfg_file", help="cfg file path", default='', type=str
    )
    args = parser.parse_args()
    cfg = OmegaConf.load(args.cfg_file)
    cfg.logging_path = os.path.join(
        cfg.data_folder, cfg.log_file_name + '.log'
    )
    cfg.data_save_path = os.path.join(
        cfg.data_folder, cfg.data_save_name + '.pkl'
    )

    # logging
    logging.basicConfig(
        level=logging.INFO, format='%(message)s', handlers=[
            logging.FileHandler(cfg.logging_path, mode='w'),
            logging.StreamHandler()
        ]
    )

    # run
    random.seed(cfg.seed)
    main(cfg)
