""" Generate a dataset of ambiguous requests and multiple choices for the robot.

To generate a rich dataset, we would like to generate ambiguous requests. The definition of ambiguity is as follows: the question is ambiguous to LM and more than one choices are correct; however, each choice must be executable by the downstream policy; this means the question can contain words that are not recognized by the downstream policy, but the multplice choices must be recognizable; otherwise, the choice must be (e) None of the above, and human is expected to provide the answer.

Request format: '$action the $adj1 $obj1 $rel the $adj2 $obj2.' adj stands for adjective, obj stands for object, and rel stands for spatial relation.

Types of ambiguities:
1. close adjective - e.g., yellow and gold, blue and cyan, square and rectangular
2. non-existing adjective - e.g., magenta, red, triangular
3. underspecified object - e.g., blue object, colored object
4. non-existing object - e.g., cup, plate
5. underspecified relation - e.g., next to the object, some objects - need to introduce API functions like get_obs_pos() such that LM learns to use it and it can be evaluated by the downstream policy

In order to generate the appropriate multiple choices automatically, we need to define the relations between words such that the sentence can be filled in with appropriate word choice. We define the following relations:
        1. eq: equivalent in meaning
        2. amb: ambiguous in meaning
        3. sem: semantically related but incorrect
Note that for the related words, we only use words that are recognized by the downstream module (CLIP), which are those defined in the demonstration datasets. Many of the related words are auto-generated by github copilot, and verified manually.

Rules for generating multiple choices:
1. if question is not ambiguous, always generate correct answers (maximum number: max_eq_mc) - either use the exact same sentence as question, or substitute with equivalent words. Then generate incorrect ones using semantically related words.
2. if question is ambiguous, generate ambiguous ones (maximum number: max_amb_mc) possible ones using ambiguous words. Then generate incorrect ones using semantically related words.
3. always add 'None of the above' as the last option.
4. for the spatial relation, if we sample equivalent or ambiguous words, use equivalent or ambiguous relations. If we sample semantic words, also use semantic relations.

Rule for generating the true answer:
1. If there exists a multiple choice of type eq (right now no more than one), always choose it.
2. Otherwise, if there is at least one of the multiple choices is type amb, randomly choose from one of them. 
3. Otherwise, meaning all multiple choices are type sem, choose None of the above.

"""

import os
import argparse
from omegaconf import OmegaConf
import pickle
import logging
import random

from agent.task import Task
from util.data import determine_true_label_type, postprocess_mc


def main(cfg):
    # Task agent
    task_agent = Task(cfg)

    # Generate data
    data = []
    data_ind = 0
    num_sem_target = int(cfg.num_data * cfg.sem_ratio)
    num_sem = 0
    while data_ind < cfg.num_data:

        # Sample request
        request, info = task_agent.sample_request()
        _, obj1, adj1, rel, obj2, adj2 = info.sample_words.values()

        # Generate context and multiple choices according to the ambiguity type
        gen_mc = task_agent.generate_mc(
            obj1, adj1, rel, obj2, adj2, info.ambiguity_type
        )

        # Prepend context
        context = cfg.question_prompt.replace('{request}', request)

        # Re-sample if generate_mc() returns None or max number of sem data is reached
        if gen_mc is None:
            continue
        else:
            mc_all, mc_types, info_mc = gen_mc

            # Determine if sampling more sem data
            exclude_sem = (num_sem == num_sem_target)
            if exclude_sem and all(mc_type == 'sem' for mc_type in mc_types):
                continue
            data_ind += 1

        # Log
        logging.info(f'# Ambiguity type: {info.ambiguity_name}')
        logging.info(f'# {data_ind}: {request}')

        # Post-process the sampled multiple choices
        mc_all, mc_types, context = postprocess_mc(
            context,
            mc_all,
            mc_types,
            cfg.mc_sigs,
            add_none_option=cfg.add_none_option,
            verbose=True,
        )
        context += cfg.answer_prompt

        # Save data
        data.append({
            'context': context,
            'mc': mc_all,
            'mc_types': mc_types,
            #   'num_amb_mc': info_mc['num_amb_mc'],
            'request_type': info.ambiguity_name
        })

        # Generate labels automatically based on request type
        true_label, true_type = determine_true_label_type(
            mc_types,
            cfg.mc_sigs,
            use_e_for_multiple_amb=cfg.use_e_for_multiple_amb,
            use_e_for_possible_amb=cfg.use_e_for_possible_amb,
            num_amb_mc_possible=info_mc['num_amb_mc_possible'],
        )
        if true_type == 'sem':
            num_sem += 1
        logging.info(f'True label: {true_label}. True type: {true_type}.\n')
        data[-1]['true_label'] = true_label
        data[-1]['true_type'] = true_type

    # Save all data
    with open(cfg.save_data_path, 'wb') as handle:
        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # Summarize
    logging.info('\n============== Summary ==============')
    logging.info(f'Number of questions generated: {len(data)}')
    logging.info(
        f'Number of questions of eq type: {len([x for x in data if x["true_type"] == "eq"])}'
    )
    logging.info(
        f'Number of questions of amb type: {len([x for x in data if x["true_type"] == "amb"])}'
    )
    logging.info(
        f'Number of questions of sem type: {len([x for x in data if x["true_type"] == "sem"])}'
    )
    logging.info(
        f'Number of repeated questions: {len(data) - len(set([x["context"] for x in data]))}'
    )
    logging.info(
        f"Number of attribute ambiguity questions: {len([x for x in data if x['request_type'] == 'attribute'])}"
    )
    logging.info(
        f"Number of spatial ambiguity questions: {len([x for x in data if x['request_type'] == 'spatial'])}"
    )
    logging.info(
        f"Number of numeric ambiguity questions: {len([x for x in data if x['request_type'] == 'numeric'])}"
    )
    count = 0
    for x in data:
        if len(set(x['mc'])) < len(x['mc']):
            count += 1
            # print(x['context'])
            # print(x['true_label'])
            # print(x['true_type'])
            # print()
    logging.info(
        f'Number of questions with repeated multiple choices: {count}\n'
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-cf", "--cfg_file", help="cfg file path", default='', type=str
    )
    args = parser.parse_args()
    if args.cfg_file == '':
        print('Using pre-defined parameters!')
        cfg = OmegaConf.create()
        cfg.num_data = 500
        cfg.num_mc_sample = 4  # number of multiple choices for each question, excluding (e) none of the above
        cfg.max_eq_mc = 1  # maximum number of multiple choices with eq combination
        cfg.max_amb_mc = 3  # maximum number of multiple choices with ambiguous combination
        cfg.amb_mc_ratio = [
            0.5, 0.3, 0.2
        ]  # split of probability of the number of ambiguous multiple choices
        assert cfg.max_amb_mc == len(cfg.amb_mc_ratio)
        cfg.data_folder = 'data/test_122022'
        cfg.question_prompt = 'Question: what should the robot do now?'
        cfg.answer_prompt = 'Answer:'  # or 'Answer (assign logprob for each option): ', or 'Choose from {a,b,c,d,e}:'
        cfg.adj_choices = None
        cfg.obj_choices = None
    else:
        cfg = OmegaConf.load(args.cfg_file)
    cfg.save_data_path = os.path.join(cfg.data_folder, 'request_data.pkl')
    cfg.logging_path = os.path.join(cfg.data_folder, 'collect_data.log')

    # logging
    logging.basicConfig(
        level=logging.INFO, format='%(message)s', handlers=[
            logging.FileHandler(cfg.logging_path, mode='w'),
            logging.StreamHandler()
        ]
    )  # overwrite

    # Seed
    random.seed(cfg.seed)

    # run
    main(cfg)
